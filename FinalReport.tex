%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Project Title : Parallel Methods in Linear Algebra
% Author        : Robert Pellegrin
% Date          : April 16, 2025
% Course        : MATH306 - Linear Algebra
% Instructor    : Dr. Spickler
% Description   : This document is the final report for the course project.
%                 It includes abstract, background, methods, results,
%                 and future work in standard research paper format.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{times} % For Times font
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{float}



% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{Final Project Report}

% Title formatting
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{2in}
    {\Huge\bfseries Parallel Methods in Linear Algebra\par}
    \vspace{1in}
    {\Large Robert Pellegrin\par}
    \vspace{0.5in}
    {\large MATH 306\\ Dr.\ Spickler \\ \today\par}
    \vfill
\end{titlepage}

% Table of contents
\newpage
\thispagestyle{empty}
\vspace*{2cm}
\begin{center}
    {\LARGE \bfseries Table of Contents}
\end{center}
\vspace{1.5cm}
\tableofcontents
\newpage


% Abstract
\newpage
\section*{Abstract}
This project explores the performance of serial and parallel implementations of two core linear algebra operations: \textbf{matrix multiplication}
and \textbf{LU decomposition}. These algorithms were implemented in both C++ and Rust, allowing for a comparison of not only parallel speedup but
also language-level performance differences. The matrix multiplication was done using the standard three-loop algorithm, while LU decomposition
was implemented with partial pivoting and rounding of very small values to zero to reduce numerical errors.

\parskip=1em

Parallel versions were created using OpenMP in C++ and the Rayon library in Rust, both of which enable shared-memory parallelism with minimal
code changes. Performance testing was conducted on square matrices of increasing sizes, with execution times recorded and averaged across
multiple runs to ensure consistency. The results were then analyzed to evaluate the benefits of parallelism and to compare how each language
handles high-performance numerical computing tasks.

\parskip=1em

By benchmarking both serial and parallel variants in each language, this project aims to provide insights into the trade-offs between ease of
implementation, execution speed, and scalability when working with computationally intensive linear algebra algorithms.

% Begin main content
\section{Background and Motivation}
Linear algebra is incredibly important in computer science and related fields, from graphics and machine learning to scientific simulations.
Two common but computationally intensive tasks in linear algebra are matrix multiplication and LU decomposition. Matrix multiplication is fundamental
in areas like computer graphics, graph theory, and neural networks, while LU decomposition helps us solve systems of equations
efficiently---something critical in science, engineering, and computing.

\parskip=1em

Even though these operations are conceptually straightforward, performing them on large matrices can become extremely slow when using a simple,
one-step-at-a-time (serial) approach. As the amount of data gets bigger, serial implementations of these algorithms can become too slow to remain practical. This is
where parallel computing becomes interesting---by running computations simultaneously on a multi-core CPU, we can potentially
achieve big improvements in speed.

\parskip=1em

This project focuses on implementing and comparing serial and parallel versions of matrix multiplication and LU decomposition. Specifically,
I'm using two popular programming languages: C++ and Rust. C++ has long been favored for performance-sensitive tasks because it offers powerful
tools for optimization. Rust, though newer, has gained popularity due to its built-in safety features and strong performance, sometimes outperforming
older languages in efficiency and speed.

\parskip=1em

By writing and benchmarking serial and parallel versions of these algorithms in both languages, the goal is to answer two key questions:

\begin{itemize}
    \item How much faster are parallel versions of these algorithms compared to their serial counterparts?
    \item Does the choice of programming language (C++ versus Rust) significantly impact the performance?
\end{itemize}

This comparison is useful because, as computing increasingly moves toward multi-core processors, it's essential to know which languages and methods can
best take advantage of this hardware. The results could help guide decisions in future projects, especially when performance is critical.


\section{Methods \& Tools}

This project involved implementing two fundamental linear algebra algorithms---\textbf{matrix multiplication} and \textbf{LU decomposition}---in both
serial and parallel forms, using two different programming languages: \textbf{C++} and \textbf{Rust}. The goal was to explore not only how parallelism
improves performance, but also how the choice of language affects execution time and overall efficiency.

\subsection*{Matrix Multiplication}

Matrix multiplication in this project was implemented using the standard algorithm involving three nested loops. Given two matrices, \( A \) and \( B \),
the resulting matrix \( C \) is computed such that each element \( C_{ij} \) is the sum of products between the elements in the \( i \)-th row of \( A \)
and the \( j \)-th column of \( B \):

\[
    C_{ij} = \sum_{k=1}^{n} A_{ik} \times B_{kj}
\]

The standard matrix multiplication algorithm has a time complexity of \( \mathcal{O}(n^3) \), where \( n \) is the size of the square matrices.
This cubic growth causes runtime to increase rapidly as matrix size increases, making parallelization especially important for maintaining reasonable
performance at larger scales.

To address this challenge, a parallel version of the algorithm was implemented by distributing the outer loop (which iterates over the rows of the
result matrix) across multiple threads. In C++, parallelization was achieved using \textbf{OpenMP}, which simplifies loop-level parallelism through
the \texttt{\#pragma omp parallel for} directive. In Rust, the \textbf{Rayon} library was used to safely and efficiently parallelize iteration using
the \texttt{par\_iter()} API.

The algorithm was tested on square matrices of various sizes (e.g., \(100 \times 100\), \(500 \times 500\), \(1000 \times 1000\), etc.), and execution
times were recorded to observe how performance scaled with input size and to evaluate the speedup achieved by the parallel implementations compared to
their serial counterparts.

\subsection*{LU Decomposition}
The LU decomposition algorithm with partial pivoting has a time complexity of \( \mathcal{O}(n^3) \), where \( n \) is the size of the square matrix.
This complexity arises from the nested loop structure used to eliminate entries below the main diagonal and construct the lower and upper triangular matrices.
As matrix size increases, the computational cost becomes substantial, making parallelization of the elimination phase especially valuable.

LU decomposition factors a matrix \( A \) into the product of a lower triangular matrix \( L \) and an upper triangular matrix \( U \). When partial
pivoting is introduced to improve numerical stability, the factorization becomes \( PA = LU \), where \( P \) is a permutation matrix representing the
row swaps made during the decomposition. Partial pivoting helps avoid divisions by small pivot values, which can otherwise lead to large rounding errors
in floating-point arithmetic.To further mitigate floating-point inaccuracies, a small threshold (e.g., \(1 \times 10^{-10}\)) was applied to round very
small computed values to zero. This helps reduce numerical noise introduced during real-number computations and ensures cleaner results.

Parallelizing LU decomposition is more complex than matrix multiplication due to data dependencies between rows, especially during pivot selection and row
elimination. In the parallel implementation, the row elimination steps that follow each pivot selection were parallelized. In C++, \textbf{OpenMP} was used
to distribute inner loop computations across threads. In Rust, \textbf{Rayon} was used to parallelize row operations using safe, efficient iterators.
Synchronization was carefully managed to avoid race conditions, which occur when multiple threads try to read from and write to shared data at the same time without proper coordination. In the context of LU decomposition, race conditions could happen when updating shared structures such as pivot rows or performing row swaps, potentially leading to incorrect results or unpredictable behavior if not properly controlled.

Although more advanced or block-based LU algorithms with improved cache utilization exist, the classical method with partial pivoting was selected for this
project due to its simplicity and suitability for evaluating serial and parallel implementations in both C++ and Rust.


\subsection*{Languages \& Libraries}

\textbf{C++} was chosen for its long-standing reputation in high-performance computing, and \textbf{Rust} for its modern design, strong memory safety guarantees,
and native support for concurrency. The tools and libraries used include:

\begin{itemize}
    \item \textbf{C++}
          \begin{itemize}
              \item GCC compiler with optimization flags (\texttt{-O2}, \texttt{-fopenmp})
              \item OpenMP for parallelism
              \item \texttt{chrono} library for timing
          \end{itemize}

    \item \textbf{Rust}
          \begin{itemize}
              \item Stable Rust compiler (\texttt{cargo})
              \item Rayon crate for data-parallelism
              \item \texttt{std::time::Instant} for timing
          \end{itemize}
    \item \textbf{Bash}
          \begin{itemize}
              \item GNU Parallel
          \end{itemize}
\end{itemize}

Both versions of the code were tested on the same machine to ensure consistent comparisons. Basic matrix generators along with various
sanity-checking functions were used to verify the correctness of the algorithms by comparing outputs from serial and parallel versions.

To ensure fair and efficient performance comparisons, compiler optimizations were enabled for both C++ and Rust builds. For the C++ programs,
optimization flags  \texttt{-O2} and \texttt{-fopenmp} were used with the GCC compiler to improve execution speed and enable parallelization via OpenMP.
In Rust, programs were compiled using the \texttt{--release} flag, which enables aggressive optimizations through LLVM, resulting in significantly faster
runtime performance compared to (default) debug builds. These optimizations were necessary for obtaining realistic benchmark results, particularly when
comparing serial and parallel execution times across both languages.

To reduce the time needed to collect results from the serial implementations of each algorithm, \textbf{GNU Parallel} was used to perform multiple test runs
simultaneously across different CPU cores. Although each individual program was still running serially, running multiple instances at once allowed better
utilization of available hardware resources during data collection, significantly reducing the total time required to benchmark all matrix sizes.
GNU Parallel made it easy to queue up many independent executions while ensuring that system load remained reasonable.

\subsection*{Performance Measurement}

Execution time was the primary metric used to compare the performance of different implementations. Each program was run multiple times, and the average
of five runs was recorded to minimize variance caused by system processes or hardware interruptions. Results were organized in tables and plotted to visualize
the differences between serial and parallel execution, as well as between C++ and Rust implementations.

To help automate the benchmarking process, a Bash script was written that incrementally increased the matrix size passed to each program. This script looped
through a range of sizes (e.g., from 1,000 to 10,000, increasing by 1,000 each time), called the appropriate executable, and captured the runtime output.
Automating this step not only ensured consistency across runs but also made it easier to test a broad range of input sizes without manually rerunning
each test.

\section{Results --- Matrix Multiplication}
% \section*{Results}
Performance testing was conducted on matrix multiplication implementations for both C++ and Rust, using serial and parallel versions. Timings were recorded for square matrices ranging from \(1000 \times 1000\) up to \(10000 \times 10000\).

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Matrix Size} & \textbf{Rust Serial (s)} & \textbf{Rust Parallel (s)} & \textbf{C++ Serial (s)} & \textbf{C++ Parallel (s)} \\
        \hline
        1,000                & 2.36                     & 0.19                       & 4                       & 0                         \\
        2,000                & 43.37                    & 1.86                       & 161                     & 2                         \\
        3,000                & 144.72                   & 6.67                       & 574                     & 9                         \\
        4,000                & 346.12                   & 16.92                      & 1160                    & 29                        \\
        5,000                & 619.36                   & 35.46                      & 2205                    & 65                        \\
        6,000                & 1144.60                  & 67.48                      & 3068                    & 128                       \\
        7,000                & 1953.34                  & 126.30                     & 4614                    & 222                       \\
        8,000                & 2961.97                  & 243.83                     & 6708                    & 350                       \\
        9,000                & 4400.20                  & 476.71                     & 23492                   & 2110                      \\
        10,000               & 6016.93                  & 692.93                     & 31935                   & 3096                      \\
        \hline
    \end{tabular}
    \caption{Matrix multiplication execution times for Rust and C++ in serial and parallel implementations.
        All timings are in seconds.}
    \label{tab:matrix-timings}
\end{table}


\subsection*{Rust Matrix Multiplication}

In the Rust serial implementation, execution time grew significantly with matrix size, consistent with the expected \( \mathcal{O}(n^3) \) complexity.
Multiplying two \(1000 \times 1000\) matrices took approximately 2.36 seconds, and multiplying \(10000 \times 10000\) matrices required about 100 minutes.

The parallel Rust implementation, using Rayon, showed substantial speedup at all tested sizes. For instance, multiplying \(10000 \times 10000\) matrices
was reduced from 100 minutes to approximately 11.5 minutes, demonstrating a major improvement. Even at smaller sizes, such as \(1000 \times 1000\), the
parallel version completed in just 0.19 seconds.

\subsection*{C++ Matrix Multiplication}

The C++ serial implementation showed a similar growth trend but required more time than Rust across all matrix sizes. Multiplying \(1000 \times 1000\) matrices
took about 4 seconds, while \(10000 \times 10000\) matrices required over 8.5 hours (approximately 532 minutes).

The parallel C++ implementation, using OpenMP, also significantly reduced execution times compared to the serial version. For instance, \(10000 \times 10000\)
matrices were completed in about 52 minutes, compared to 532 minutes serially. However, even with parallelization, the C++ implementation remained slower
than the Rust parallel version at all tested sizes.

\subsection*{Comparison and Observations}

Across all matrix sizes and implementations, Rust consistently outperformed C++ in both serial and parallel executions. Rust's serial version was significantly
faster than the C++ serial version, and its parallel implementation achieved even greater relative speedup compared to C++ with OpenMP.

These results suggest that Rust, combined with the Rayon library for data-parallelism, offers highly competitive—and in this case, superior—performance for
matrix multiplication tasks compared to traditional C++ approaches. Additionally, the simplicity of writing parallel code in Rust with Rayon compared to the
more manual setup of OpenMP further highlights Rust's strength as a language for high-performance computing.


\section{Results --- LU Decomposition}

Performance testing was conducted on LU decomposition implementations for both C++ and Rust, in both serial and parallel forms. Timings were recorded for square matrices ranging in size from \(1000 \times 1000\) up to \(10000 \times 10000\).

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Matrix Size} & \textbf{Rust Serial (s)} & \textbf{Rust Parallel (s)} & \textbf{C++ Serial (s)} & \textbf{C++ Parallel (s)} \\
        \hline
        1,000                & 1.33                     & 0.25                       & 2                       & 0                         \\
        2,000                & 11.39                    & 1.01                       & 18                      & 1                         \\
        3,000                & 39.20                    & 3.45                       & 60                      & 6                         \\
        4,000                & 79.38                    & 8.07                       & 132                     & 15                        \\
        5,000                & 143.09                   & 15.63                      & 241                     & 30                        \\
        6,000                & 232.67                   & 26.62                      & 385                     & 52                        \\
        7,000                & 353.85                   & 41.75                      & 580                     & 82                        \\
        8,000                & 499.40                   & 61.86                      & 804                     & 122                       \\
        9,000                & 693.20                   & 87.32                      & 1120                    & 174                       \\
        10,000               & 917.97                   & 118.78                     & 1492                    & 239                       \\
        \hline
    \end{tabular}
    \caption{LU decomposition execution times for Rust and C++ in serial and parallel implementations. All timings are in seconds.}
    \label{tab:lu-timings}
\end{table}


\subsection*{Rust LU Decomposition}

In the Rust serial implementation, execution time increased rapidly with matrix size, consistent with the expected \( \mathcal{O}(n^3) \) complexity.
Decomposing a \(1000 \times 1000\) matrix took approximately 1.33 seconds, while a \(10000 \times 10000\) matrix required about 15 minutes and 18 seconds.

The parallel Rust implementation, using the Rayon library, significantly reduced computation times across all tested matrix sizes. At \(10000 \times 10000\),
decomposition completed in under 2 minutes, a major improvement over the serial version. Even at smaller sizes, such as \(1000 \times 1000\), the parallel
version completed in approximately 0.25 seconds.

\subsection*{C++ LU Decomposition}

The C++ serial LU decomposition implementation also exhibited a steep increase in execution time with matrix size. Decomposing a \(1000 \times 1000\) matrix
took about 2 seconds, while a \(10000 \times 10000\) matrix required approximately 24 minutes and 52 seconds.

Parallelizing the C++ implementation using OpenMP significantly improved performance, particularly for larger matrices. For example, the decomposition of
a \(10000 \times 10000\) matrix was completed in approximately 4 minutes in the parallel version, compared to nearly 25 minutes serially.

\subsection*{Comparison and Observations}

Rust consistently outperformed C++ in both serial and parallel LU decomposition timings. The Rust serial implementation was faster than the C++ serial
implementation across all matrix sizes, and the parallel Rust version achieved even greater relative speedup compared to C++ with OpenMP.

For large matrices, the Rust parallel implementation completed decomposition tasks in roughly half the time required by the C++ parallel implementation.
The simplicity of parallelizing LU decomposition in Rust using Rayon, compared to the manual threading model required with OpenMP in C++, further highlights
Rust's strength as a high-performance parallel programming language.

These results emphasize the importance of parallelization when scaling LU decomposition to larger matrix sizes and demonstrate the competitive advantage offered
by Rust's concurrency ecosystem.

\section{Summary}

This project examined the performance of serial and parallel implementations of matrix multiplication and LU decomposition using both C++ and Rust.
Standard algorithms were employed for both tasks, with parallelization achieved through OpenMP in C++ and the Rayon library in Rust. Extensive testing
across a range of matrix sizes confirmed the expected \( \mathcal{O}(n^3) \) time complexity for both operations.

Parallel implementations in both languages demonstrated substantial improvements over their serial counterparts. However, Rust consistently outperformed
C++ in both serial and parallel scenarios for matrix multiplication and LU decomposition. The Rust parallel implementations, in particular, achieved notable
speedups, completing tasks in significantly less time than the equivalent C++ OpenMP versions, even at large matrix sizes.

These findings highlight not only the benefits of parallelization for computationally intensive linear algebra operations but also the strength of Rust as
a high-performance, parallel-capable language. With safe concurrency features and powerful libraries like Rayon, Rust proves to be a highly competitive option
for scientific and engineering applications traditionally dominated by C++. Overall, the project demonstrates that thoughtful application of parallelism and
choice of programming tools can dramatically improve the efficiency of fundamental linear algebra computations.


\section{Future Work}
While this project focused on matrix multiplication and LU decomposition, there are many other linear algebra algorithms that could benefit significantly
from parallelization. Expanding the scope of this work to include additional algorithms would offer a more comprehensive understanding of where parallel
computing provides the most value in scientific and mathematical computing.

\parskip=1em

One natural extension would be to explore \textbf{QR decomposition} and \textbf{Cholesky decomposition}, both of which are used in numerical analysis,
machine learning, and solving systems of equations. These algorithms involve matrix transformations that are computationally intensive and contain
steps that could be parallelized similarly to LU decomposition. Another candidate is the \textbf{Singular Value Decomposition (SVD)}, which is widely
used in data science and dimensionality reduction. Although more complex, it presents a great opportunity for parallel optimization due to the large number
of operations involved.

\parskip=1em

Beyond algorithm selection, there are also more advanced parallelization techniques that could be explored. In this project, shared-memory
parallelism was used via OpenMP in C++ and the Rayon library in Rust. However, these approaches are limited by the number of CPU cores available.
To push performance further, especially for large matrix sizes, \textbf{GPU acceleration} using technologies like \textbf{NVIDIA CUDA} could be
highly effective. GPUs are well-suited for linear algebra due to their high number of processing cores and fast context switching, allowing massive
numbers of computations to run in parallel.

\parskip=1em

In the Rust ecosystem, GPU programming is still emerging, but there are crates like \texttt{cust} (a CUDA wrapper) that can be used to write Rust
programs that run directly on the GPU. Exploring how GPU-based matrix operations in Rust compare to traditional CPU-based ones would be a valuable
next step. Similarly, \textbf{SIMD} (Single Instruction, Multiple Data) is another performance optimization strategy worth considering. SIMD allows
a single instruction to operate on multiple data points at once, which is perfect for vector and matrix computations. Rust has libraries like
\texttt{wide} and \texttt{std::simd} (which is still in development) that provide abstractions for SIMD programming and could significantly boost the
performance of both serial and parallel implementations.

\parskip=1em

Another area of interest would be to look at \textbf{distributed computing} approaches for extremely large datasets that don't fit into memory on a
single machine. While this project didn't include MPI, revisiting distributed memory parallelism could provide insight into scaling linear algebra computations beyond a single workstation.

\parskip=1em

Lastly, deeper numerical accuracy analysis and profiling could be explored. Comparing how different languages and approaches handle
floating-point errors or identifying performance bottlenecks at the instruction level using profiling tools like \texttt{gprof}, \texttt{perf}, or
Rust's \texttt{flamegraph} crate could provide more technical depth and further guide optimization efforts.

\parskip=1em

In summary, while this project lays a solid foundation for understanding parallelism in matrix operations, there is much room for future work in exploring
additional algorithms, more advanced hardware acceleration techniques, and deeper performance analysis.


% Begin Bibliography
\newpage
\begin{thebibliography}{9}

    \bibitem{gnuParallel}
    Ole Tange, \textit{GNU Parallel: The Command-Line Power Tool}, ;login: The USENIX Magazine, February 2011. \url{https://www.gnu.org/software/parallel/}

    \bibitem{openmpSpec2020}
    OpenMP Architecture Review Board, \textit{OpenMP Application Programming Interface Version 5.1},
    \url{https://www.openmp.org/specifications/}, 2020.

    \bibitem{rayonGitHub}
    Niko Matsakis and Rayon Contributors, \textit{Rayon: A data parallelism library for Rust},
    \url{https://github.com/rayon-rs/rayon}, Accessed 2025.

    \bibitem{rayonDocs}
    Rayon Contributors, \textit{Rayon Crate Documentation}, \url{https://docs.rs/rayon/latest/rayon/}, Accessed 2025.

\end{thebibliography}

\pagebreak

% Appendix
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\end{document}
